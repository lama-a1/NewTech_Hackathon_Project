{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq5xXWvBzZsOHzKXz2Pwa3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lama-a1/NewTech_Hackathon_Project/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIP6JRvjtXoM",
        "outputId": "5f1fe195-63e6-41d9-d2a2-b494fe10e1c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Hour    HR  O2Sat  Resp   Temp   MAP  Lactate  SepsisLabel  PatientID_tmp  \\\n",
            "0     0  65.0  100.0  16.5  35.78  72.0      1.9            0              1   \n",
            "1     1  65.0  100.0  16.5  35.78  72.0      1.9            0              1   \n",
            "2     2  78.0  100.0  16.5  35.78  42.5      1.9            0              1   \n",
            "3     3  73.0  100.0  17.0  35.78  42.5      1.9            0              1   \n",
            "4     4  70.0  100.0  14.0  35.78  74.0      1.9            0              1   \n",
            "\n",
            "   is_baseline  ...  Resp_dev  Temp_dev    MAP_dev   Lactate_dev      HR_z  \\\n",
            "0         True  ...  1.458333 -0.050833  -1.541667  2.220446e-16 -0.757842   \n",
            "1         True  ...  1.458333 -0.050833  -1.541667  2.220446e-16 -0.757842   \n",
            "2         True  ...  1.458333 -0.050833 -31.041667  2.220446e-16  1.180246   \n",
            "3         True  ...  1.958333 -0.050833 -31.041667  2.220446e-16  0.434828   \n",
            "4         True  ... -1.041667 -0.050833   0.458333  2.220446e-16 -0.012424   \n",
            "\n",
            "   O2Sat_z    Resp_z    Temp_z     MAP_z  Lactate_z  \n",
            "0      0.0  1.117619 -0.288675 -0.094716        0.0  \n",
            "1      0.0  1.117619 -0.288675 -0.094716        0.0  \n",
            "2      0.0  1.117619 -0.288675 -1.907127        0.0  \n",
            "3      0.0  1.500803 -0.288675 -1.907127        0.0  \n",
            "4      0.0 -0.798300 -0.288675  0.028159        0.0  \n",
            "\n",
            "[5 rows x 40 columns]\n",
            "Saved PreprocessedDataset.csv (78359, 40)\n",
            "Patients: 2000\n",
            "Columns: 40\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# =========================\n",
        "# 1) Load raw dataset\n",
        "# =========================\n",
        "df = pd.read_csv(\"Dataset.csv\")\n",
        "\n",
        "LABEL = \"SepsisLabel\"\n",
        "\n",
        "# Keep only needed columns if they exist\n",
        "base_cols = [\"Hour\", \"HR\", \"O2Sat\", \"Resp\", \"Temp\", \"MAP\", \"Lactate\", LABEL]\n",
        "base_cols = [c for c in base_cols if c in df.columns]\n",
        "df = df[base_cols].copy()\n",
        "\n",
        "# =========================\n",
        "# 2) Ensure Hour is numeric\n",
        "# =========================\n",
        "df[\"Hour\"] = pd.to_numeric(df[\"Hour\"], errors=\"coerce\")\n",
        "\n",
        "# =========================\n",
        "# 3) Build temporary PatientID (when Hour resets to 0 -> new patient)\n",
        "# =========================\n",
        "# If Hour has NaN we treat as -1 so it won't be counted as reset-to-zero\n",
        "df[\"PatientID_tmp\"] = (df[\"Hour\"].fillna(-1).eq(0)).cumsum()\n",
        "\n",
        "# =========================\n",
        "# 4) Sample patients (for hackathon speed)\n",
        "# =========================\n",
        "N_PATIENTS = 2000\n",
        "patient_ids = df[\"PatientID_tmp\"].drop_duplicates().head(N_PATIENTS)\n",
        "df = df[df[\"PatientID_tmp\"].isin(patient_ids)].copy()\n",
        "\n",
        "# =========================\n",
        "# 5) Sort within each patient by time\n",
        "# =========================\n",
        "df.sort_values([\"PatientID_tmp\", \"Hour\"], inplace=True)\n",
        "\n",
        "# =========================\n",
        "# 6) Missing values handling (FFill + BFill per patient)\n",
        "# =========================\n",
        "feat_cols = [c for c in df.columns if c not in [LABEL, \"PatientID_tmp\"]]\n",
        "df[feat_cols] = df.groupby(\"PatientID_tmp\")[feat_cols].transform(lambda x: x.ffill().bfill())\n",
        "\n",
        "# =========================\n",
        "# 7) Baseline flag (first 12 hours: 0..11)\n",
        "# =========================\n",
        "df[\"is_baseline\"] = df[\"Hour\"] <= 11\n",
        "\n",
        "# ============================================================\n",
        "# 8) NEW: Delta Features (change from previous hour per patient)\n",
        "# ============================================================\n",
        "# These capture \"trend\" which is crucial for sepsis early detection\n",
        "vitals = [c for c in [\"HR\", \"O2Sat\", \"Resp\", \"Temp\", \"MAP\", \"Lactate\"] if c in df.columns]\n",
        "\n",
        "for col in vitals:\n",
        "    df[f\"d_{col}\"] = df.groupby(\"PatientID_tmp\")[col].diff()  # (t - t-1)\n",
        "    # Fill first diff in each patient with 0 (no previous hour)\n",
        "    df[f\"d_{col}\"] = df[f\"d_{col}\"].fillna(0)\n",
        "# =========================\n",
        "# 9) Baseline Normalization (patient-centric deviation)\n",
        "# =========================\n",
        "baseline = df[df[\"is_baseline\"]].copy()\n",
        "\n",
        "baseline_means = (\n",
        "    baseline.groupby(\"PatientID_tmp\")[vitals]\n",
        "    .mean()\n",
        "    .add_prefix(\"base_mean_\")\n",
        "    .reset_index()\n",
        ")\n",
        "df = df.merge(baseline_means, on=\"PatientID_tmp\", how=\"left\")\n",
        "\n",
        "baseline_stds = (\n",
        "    baseline.groupby(\"PatientID_tmp\")[vitals]\n",
        "    .std()\n",
        "    .replace(0, np.nan)\n",
        "    .add_prefix(\"base_std_\")\n",
        "    .reset_index()\n",
        ")\n",
        "df = df.merge(baseline_stds, on=\"PatientID_tmp\", how=\"left\")\n",
        "\n",
        "for col in vitals:\n",
        "    df[f\"{col}_dev\"] = df[col] - df[f\"base_mean_{col}\"]\n",
        "\n",
        "for col in vitals:\n",
        "    std_col = f\"base_std_{col}\"\n",
        "    z_col = f\"{col}_z\"\n",
        "    df[z_col] = (df[col] - df[f\"base_mean_{col}\"]) / df[std_col]\n",
        "    df[z_col] = df[z_col].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "# =========================\n",
        "# 11) Save preprocessed dataset\n",
        "# =========================\n",
        "print(df.head())  # <-- fixed\n",
        "df.to_csv(\"PreprocessedDataset.csv\", index=False)\n",
        "print(\"Saved PreprocessedDataset.csv\", df.shape)\n",
        "print(\"Patients:\", df[\"PatientID_tmp\"].nunique())\n",
        "print(\"Columns:\", len(df.columns))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# GRU Multi-task Training (Multi-step Forecast 6h + Sepsis Risk 6h)\n",
        "# For PreprocessedDataset.csv (already contains 40 preprocessed columns)\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# -----------------------------\n",
        "# 0) SETTINGS\n",
        "# -----------------------------\n",
        "CSV_PATH = \"/content/PreprocessedDataset.csv\"   # عدلي المسار إذا لازم\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Windows\n",
        "SEQ_LEN = 12          # input: last 12 hours\n",
        "HORIZON = 6           # forecast: next 6 hours (t+1 .. t+6)\n",
        "RISK_HORIZON = 6      # risk: sepsis within next 6 hours (same window)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 20\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "PATIENCE = 4\n",
        "\n",
        "# Task weights\n",
        "LAMBDA_FORECAST = 1.0\n",
        "LAMBDA_RISK = 1.0\n",
        "\n",
        "THRESH = 0.5\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# -----------------------------\n",
        "# 1) LOAD DATA\n",
        "# -----------------------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "PID_COL = \"PatientID_tmp\"\n",
        "TIME_COL = \"Hour\"\n",
        "LABEL_COL = \"SepsisLabel\"\n",
        "\n",
        "VITALS = [\"HR\", \"O2Sat\", \"Resp\", \"Temp\", \"MAP\", \"Lactate\"]\n",
        "\n",
        "required = [PID_COL, TIME_COL, LABEL_COL] + VITALS\n",
        "missing_req = [c for c in required if c not in df.columns]\n",
        "if missing_req:\n",
        "    raise ValueError(f\"Missing required columns: {missing_req}\")\n",
        "\n",
        "df[TIME_COL] = pd.to_numeric(df[TIME_COL], errors=\"coerce\")\n",
        "df = df.sort_values([PID_COL, TIME_COL]).reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) FEATURES (use preprocessed columns directly)\n",
        "# Recommended: VITALS + deltas + deviations\n",
        "# -----------------------------\n",
        "FEATURES = (\n",
        "    VITALS\n",
        "    + [f\"d_{c}\" for c in VITALS]\n",
        "    + [f\"{c}_dev\" for c in VITALS]\n",
        "    # Optional later:\n",
        "    # + [f\"{c}_z\" for c in VITALS]\n",
        ")\n",
        "\n",
        "missing_feat = [c for c in FEATURES if c not in df.columns]\n",
        "if missing_feat:\n",
        "    raise ValueError(f\"Missing feature columns in preprocessed file: {missing_feat}\")\n",
        "\n",
        "TARGETS = VITALS  # we forecast these vitals for next 6 hours\n",
        "\n",
        "# -----------------------------\n",
        "# 3) PATIENT-LEVEL SPLIT\n",
        "# -----------------------------\n",
        "patient_ids = df[PID_COL].unique()\n",
        "np.random.shuffle(patient_ids)\n",
        "\n",
        "n = len(patient_ids)\n",
        "train_ids = set(patient_ids[: int(0.70 * n)])\n",
        "val_ids   = set(patient_ids[int(0.70 * n): int(0.85 * n)])\n",
        "test_ids  = set(patient_ids[int(0.85 * n):])\n",
        "\n",
        "train_df = df[df[PID_COL].isin(train_ids)].copy()\n",
        "val_df   = df[df[PID_COL].isin(val_ids)].copy()\n",
        "test_df  = df[df[PID_COL].isin(test_ids)].copy()\n",
        "\n",
        "# -----------------------------\n",
        "# 4) STANDARDIZE FEATURES + TARGETS using TRAIN only\n",
        "# -----------------------------\n",
        "feat_mean = train_df[FEATURES].mean()\n",
        "feat_std  = train_df[FEATURES].std().replace(0, 1.0)\n",
        "\n",
        "tgt_mean = train_df[TARGETS].mean()\n",
        "tgt_std  = train_df[TARGETS].std().replace(0, 1.0)\n",
        "\n",
        "def standardize(dfx: pd.DataFrame) -> pd.DataFrame:\n",
        "    dfx = dfx.copy()\n",
        "    dfx[FEATURES] = (dfx[FEATURES] - feat_mean) / feat_std\n",
        "    dfx[TARGETS]  = (dfx[TARGETS]  - tgt_mean)  / tgt_std\n",
        "\n",
        "    dfx[FEATURES] = dfx[FEATURES].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    dfx[TARGETS]  = dfx[TARGETS].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "    return dfx\n",
        "\n",
        "train_df = standardize(train_df)\n",
        "val_df   = standardize(val_df)\n",
        "test_df  = standardize(test_df)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) WINDOWED SAMPLES (Multi-step)\n",
        "# X: last SEQ_LEN hours features\n",
        "# y_forecast: vitals for next 6 hours => shape [HORIZON, 6]\n",
        "# y_risk: max SepsisLabel in the SAME next 6 hours window\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class Sample:\n",
        "    x: np.ndarray               # [SEQ_LEN, F]\n",
        "    y_forecast: np.ndarray      # [HORIZON, 6]\n",
        "    y_risk: float               # 0/1\n",
        "\n",
        "def make_samples(dfx: pd.DataFrame) -> List[Sample]:\n",
        "    samples = []\n",
        "    for pid, g in dfx.groupby(PID_COL):\n",
        "        g = g.sort_values(TIME_COL).reset_index(drop=True)\n",
        "\n",
        "        X = g[FEATURES].values.astype(np.float32)\n",
        "        Y = g[TARGETS].values.astype(np.float32)\n",
        "        L = g[LABEL_COL].values.astype(np.float32)\n",
        "\n",
        "        T = len(g)\n",
        "\n",
        "        # Need: SEQ_LEN history + HORIZON future available\n",
        "        # We predict future steps t+1..t+HORIZON, so last valid t is T - HORIZON - 1\n",
        "        last_t = T - HORIZON - 1\n",
        "        for t in range(SEQ_LEN - 1, last_t + 1):\n",
        "            x_win = X[t-(SEQ_LEN-1): t+1]             # [SEQ_LEN, F]\n",
        "\n",
        "            # Multi-step targets: next HORIZON rows\n",
        "            y_future = Y[t+1 : t+1+HORIZON]           # [HORIZON, 6]\n",
        "\n",
        "            # Risk: sepsis occurs in same future window (t+1..t+HORIZON)\n",
        "            y_risk = float(np.max(L[t+1 : t+1+HORIZON]))\n",
        "\n",
        "            samples.append(Sample(x_win, y_future, y_risk))\n",
        "\n",
        "    return samples\n",
        "\n",
        "train_samples = make_samples(train_df)\n",
        "val_samples   = make_samples(val_df)\n",
        "test_samples  = make_samples(test_df)\n",
        "\n",
        "print(\"Train/Val/Test samples:\", len(train_samples), len(val_samples), len(test_samples))\n",
        "print(\"Features:\", len(FEATURES))\n",
        "\n",
        "class SepsisDataset(Dataset):\n",
        "    def __init__(self, samples: List[Sample]):\n",
        "        self.samples = samples\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        x = torch.from_numpy(s.x)                            # [SEQ_LEN, F]\n",
        "        y_f = torch.from_numpy(s.y_forecast)                 # [HORIZON, 6]\n",
        "        y_r = torch.tensor([s.y_risk], dtype=torch.float32)  # [1]\n",
        "        return x, y_f, y_r\n",
        "\n",
        "train_loader = DataLoader(SepsisDataset(train_samples), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(SepsisDataset(val_samples),   batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(SepsisDataset(test_samples),  batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) CLASS IMBALANCE (pos_weight)\n",
        "# -----------------------------\n",
        "y_train_cls = np.array([s.y_risk for s in train_samples], dtype=np.float32)\n",
        "pos = float((y_train_cls == 1).sum())\n",
        "neg = float((y_train_cls == 0).sum())\n",
        "pos_weight = torch.tensor([neg / max(pos, 1.0)], dtype=torch.float32).to(DEVICE)\n",
        "print(\"Risk positives:\", int(pos), \"negatives:\", int(neg), \"pos_weight:\", pos_weight.item())\n",
        "\n",
        "# -----------------------------\n",
        "# 7) MODEL: GRU Multi-task (Multi-step Forecast)\n",
        "# Forecast head outputs: [B, HORIZON, 6]\n",
        "# -----------------------------\n",
        "class GRUMultiTaskMultiStep(nn.Module):\n",
        "    def __init__(self, n_features: int, hidden: int = 128, num_layers: int = 1, dropout: float = 0.15):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=n_features,\n",
        "            hidden_size=hidden,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.LayerNorm(hidden),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Forecast head: produce HORIZON*len(VITALS) then reshape to [HORIZON, 6]\n",
        "        self.forecast_head = nn.Sequential(\n",
        "            nn.Linear(hidden, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, HORIZON * len(TARGETS))\n",
        "        )\n",
        "\n",
        "        # Risk head\n",
        "        self.risk_head = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)  # logits\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)          # [B, SEQ_LEN, H]\n",
        "        last = out[:, -1, :]          # [B, H]\n",
        "        z = self.shared(last)\n",
        "\n",
        "        y_flat = self.forecast_head(z)                    # [B, HORIZON*6]\n",
        "        y_fore = y_flat.view(-1, HORIZON, len(TARGETS))   # [B, HORIZON, 6]\n",
        "\n",
        "        y_logit = self.risk_head(z)                       # [B, 1]\n",
        "        return y_fore, y_logit\n",
        "\n",
        "model = GRUMultiTaskMultiStep(n_features=len(FEATURES)).to(DEVICE)\n",
        "\n",
        "# Losses\n",
        "mse_loss = nn.MSELoss()\n",
        "bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# -----------------------------\n",
        "# 8) METRICS\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def eval_metrics(all_probs, all_true):\n",
        "    all_probs = np.asarray(all_probs)\n",
        "    all_true = np.asarray(all_true).astype(int)\n",
        "    preds = (all_probs >= THRESH).astype(int)\n",
        "    metrics = {\n",
        "        \"acc\": accuracy_score(all_true, preds),\n",
        "        \"f1\": f1_score(all_true, preds, zero_division=0),\n",
        "        \"precision\": precision_score(all_true, preds, zero_division=0),\n",
        "        \"recall\": recall_score(all_true, preds, zero_division=0),\n",
        "    }\n",
        "    metrics[\"auc\"] = roc_auc_score(all_true, all_probs) if len(np.unique(all_true)) > 1 else None\n",
        "    return metrics\n",
        "\n",
        "def run_epoch(loader, train: bool):\n",
        "    model.train(train)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_mse = 0.0\n",
        "    total_bce = 0.0\n",
        "\n",
        "    all_probs = []\n",
        "    all_true = []\n",
        "\n",
        "    for x, y_f, y_r in loader:\n",
        "        x = x.to(DEVICE)     # [B, SEQ_LEN, F]\n",
        "        y_f = y_f.to(DEVICE) # [B, HORIZON, 6]\n",
        "        y_r = y_r.to(DEVICE) # [B, 1]\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        pred_f, pred_logit = model(x)  # pred_f: [B, HORIZON, 6]\n",
        "\n",
        "        loss_f = mse_loss(pred_f, y_f)         # multi-step MSE\n",
        "        loss_r = bce_loss(pred_logit, y_r)\n",
        "\n",
        "        loss = LAMBDA_FORECAST * loss_f + LAMBDA_RISK * loss_r\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        total_mse  += loss_f.item() * x.size(0)\n",
        "        total_bce  += loss_r.item() * x.size(0)\n",
        "\n",
        "        probs = torch.sigmoid(pred_logit).detach().cpu().numpy().ravel()\n",
        "        true  = y_r.detach().cpu().numpy().ravel()\n",
        "        all_probs.append(probs)\n",
        "        all_true.append(true)\n",
        "\n",
        "    n = len(loader.dataset)\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "    all_true  = np.concatenate(all_true)\n",
        "\n",
        "    met = eval_metrics(all_probs, all_true)\n",
        "    return {\n",
        "        \"loss\": total_loss / n,\n",
        "        \"mse\": total_mse / n,\n",
        "        \"bce\": total_bce / n,\n",
        "        **met\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# 9) TRAIN LOOP (early stopping)\n",
        "# -----------------------------\n",
        "best_val = float(\"inf\")\n",
        "best_state = None\n",
        "bad = 0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    tr = run_epoch(train_loader, train=True)\n",
        "    va = run_epoch(val_loader, train=False)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"Train loss={tr['loss']:.4f} mse={tr['mse']:.4f} bce={tr['bce']:.4f} \"\n",
        "        f\"auc={tr['auc']} f1={tr['f1']:.3f} recall={tr['recall']:.3f} | \"\n",
        "        f\"Val loss={va['loss']:.4f} mse={va['mse']:.4f} bce={va['bce']:.4f} \"\n",
        "        f\"auc={va['auc']} f1={va['f1']:.3f} recall={va['recall']:.3f}\"\n",
        "    )\n",
        "\n",
        "    if va[\"loss\"] < best_val:\n",
        "        best_val = va[\"loss\"]\n",
        "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        bad = 0\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= PATIENCE:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "# -----------------------------\n",
        "# 10) TEST EVALUATION\n",
        "# -----------------------------\n",
        "te = run_epoch(test_loader, train=False)\n",
        "print(\"\\nTEST RESULTS:\")\n",
        "for k, v in te.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 11) INFERENCE helper: predict next 6 hours + risk\n",
        "# Outputs are de-standardized back to original units\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def predict_next_6h(pid: int, current_hour: int) -> Tuple[np.ndarray, float]:\n",
        "    g_std = test_df[test_df[PID_COL] == pid].sort_values(TIME_COL).reset_index(drop=True)\n",
        "    if g_std.empty:\n",
        "        raise ValueError(\"Patient not found in test_df (try a different pid).\")\n",
        "\n",
        "    idx_list = g_std.index[g_std[TIME_COL] == current_hour].tolist()\n",
        "    if not idx_list:\n",
        "        raise ValueError(\"Hour not found for this patient in test set.\")\n",
        "    idx = idx_list[0]\n",
        "\n",
        "    if idx < SEQ_LEN - 1:\n",
        "        raise ValueError(f\"Need at least {SEQ_LEN} hours history.\")\n",
        "\n",
        "    x_win = g_std.loc[idx-(SEQ_LEN-1): idx, FEATURES].values.astype(np.float32)\n",
        "    x = torch.from_numpy(x_win).unsqueeze(0).to(DEVICE)  # [1, SEQ_LEN, F]\n",
        "\n",
        "    pred_f_std, pred_logit = model(x)  # pred_f_std: [1, HORIZON, 6]\n",
        "    risk = torch.sigmoid(pred_logit).item()\n",
        "\n",
        "    pred_f_std = pred_f_std.squeeze(0).cpu().numpy()  # [HORIZON, 6]\n",
        "\n",
        "    # De-standardize per vital\n",
        "    mean = tgt_mean.values.reshape(1, -1)  # [1,6]\n",
        "    std  = tgt_std.values.reshape(1, -1)   # [1,6]\n",
        "    pred_f = pred_f_std * std + mean       # [HORIZON, 6] in original units\n",
        "\n",
        "    return pred_f, risk\n",
        "\n",
        "print(\"\\nReady ✅ call: predict_next_6h(pid, hour)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxBe1aaV8_vd",
        "outputId": "7cea92b6-f194-4ad0-ebab-874355232af2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val/Test samples: 30269 7343 7294\n",
            "Features: 18\n",
            "Risk positives: 805 negatives: 29464 pos_weight: 36.60124206542969\n",
            "Epoch 01 | Train loss=148.7602 mse=147.4075 bce=1.3527 auc=0.5172445203157701 f1=0.052 recall=0.468 | Val loss=2.8081 mse=1.4644 bce=1.3437 auc=0.6389148711743993 f1=0.048 recall=1.000\n",
            "Epoch 02 | Train loss=2.6744 mse=1.3185 bce=1.3559 auc=0.5106166615792216 f1=0.050 recall=0.528 | Val loss=1.6078 mse=0.3067 bce=1.3011 auc=0.6418264383327905 f1=0.042 recall=0.056\n",
            "Epoch 03 | Train loss=2.5387 mse=1.1889 bce=1.3499 auc=0.5050793430618774 f1=0.050 recall=0.447 | Val loss=1.6353 mse=0.3351 bce=1.3002 auc=0.6383451998697007 f1=0.041 recall=0.056\n",
            "Epoch 04 | Train loss=2.4557 mse=1.1165 bce=1.3392 auc=0.5503171993868083 f1=0.058 recall=0.461 | Val loss=1.5501 mse=0.2559 bce=1.2942 auc=0.6496300432779561 f1=0.064 recall=0.806\n",
            "Epoch 05 | Train loss=2.4214 mse=1.0866 bce=1.3348 auc=0.5641880269089303 f1=0.058 recall=0.550 | Val loss=1.5227 mse=0.2415 bce=1.2813 auc=0.6516520080040951 f1=0.072 recall=0.194\n",
            "Epoch 06 | Train loss=2.3542 mse=1.0349 bce=1.3193 auc=0.5984032730541365 f1=0.072 recall=0.496 | Val loss=1.5258 mse=0.2521 bce=1.2737 auc=0.6524578466502241 f1=0.068 recall=0.733\n",
            "Epoch 07 | Train loss=2.3311 mse=1.0192 bce=1.3119 auc=0.6093494239944145 f1=0.073 recall=0.512 | Val loss=1.5519 mse=0.2898 bce=1.2621 auc=0.6537236105294181 f1=0.065 recall=0.617\n",
            "Epoch 08 | Train loss=2.3157 mse=1.0158 bce=1.3000 auc=0.6251241645768792 f1=0.077 recall=0.513 | Val loss=1.6832 mse=0.3727 bce=1.3105 auc=0.6489560550359098 f1=0.058 recall=0.933\n",
            "Epoch 09 | Train loss=2.2899 mse=0.9904 bce=1.2995 auc=0.6244092380131644 f1=0.073 recall=0.583 | Val loss=1.6440 mse=0.3794 bce=1.2646 auc=0.6479652380287588 f1=0.067 recall=0.678\n",
            "Early stopping.\n",
            "\n",
            "TEST RESULTS:\n",
            "loss: 1.4860572576980444\n",
            "mse: 0.220322769236362\n",
            "bce: 1.265734481654626\n",
            "acc: 0.8871675349602413\n",
            "f1: 0.15762538382804503\n",
            "precision: 0.09783989834815757\n",
            "recall: 0.4052631578947368\n",
            "auc: 0.7422871473447131\n",
            "\n",
            "Ready ✅ call: predict_next_6h(pid, hour)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "artifact = {\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"FEATURES\": FEATURES,\n",
        "    \"VITALS\": VITALS,\n",
        "    \"SEQ_LEN\": SEQ_LEN,\n",
        "    \"HORIZON\": HORIZON,\n",
        "    \"RISK_HORIZON\": RISK_HORIZON,\n",
        "    \"feat_mean\": feat_mean.values.astype(np.float32),\n",
        "    \"feat_std\":  feat_std.values.astype(np.float32),\n",
        "    \"tgt_mean\":  tgt_mean.values.astype(np.float32),\n",
        "    \"tgt_std\":   tgt_std.values.astype(np.float32),\n",
        "}\n",
        "torch.save(artifact, \"sepsis_gru_multistep_artifact.pt\")\n",
        "print(\"✅ Saved: sepsis_gru_multistep_artifact.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZHBY4NpKmxq",
        "outputId": "926310a4-bd97-4ebf-8823-e1d36cad44c5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: sepsis_gru_multistep_artifact.pt\n"
          ]
        }
      ]
    }
  ]
}